{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02ce4150",
   "metadata": {},
   "source": [
    "### 문제 4-1 : OpenAI에서 Ollama Qwen3로 RAG 시스템 변경하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21a0453a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " === base 작업 === \n",
      " === base 작업 완료 === \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "기존 코드\n",
    "'''\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "print(\" === base 작업 === \")\n",
    "load_dotenv()\n",
    "\n",
    "# 문서 로드\n",
    "loader = PyPDFLoader(\"./data/콘텐츠분쟁해결_사례.pdf\")\n",
    "\n",
    "# 문자열 분할 \n",
    "pythontext_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,       \n",
    "    chunk_overlap=300,      \n",
    "    separators=[\n",
    "        \"\\n【사건개요】\",   \n",
    "        \"\\n【쟁점사항】\",   \n",
    "        \"\\n【처리경위】\",  \n",
    "        \"\\n【처리결과】\",    \n",
    "        \"\\n■\", \"\\n\\n\", \"\\n\", \".\", \" \", \"\"\n",
    "    ]\n",
    ")\n",
    "chunks = pythontext_splitter.split_documents(loader.load())\n",
    "\n",
    "# 임베드 ai 설정\n",
    "pythonembeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\", \n",
    "    dimensions=1536                  \n",
    ")\n",
    "\n",
    "# 벡터 설정\n",
    "vectorstore = FAISS.from_documents(chunks, pythonembeddings)\n",
    "python_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",        \n",
    "    search_kwargs={\"k\": 5}         \n",
    ")\n",
    "\n",
    "# ai 설정\n",
    "pythonllm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",                \n",
    "    temperature=0.2,               \n",
    "    max_tokens=2000                 \n",
    ")\n",
    "print(\" === base 작업 완료 === \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04fdebc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "질문  : BlueJ에서 객체를 생성하는 방법은 무엇인가요?\n",
      "<think>\n",
      "Okay, the user is asking about how to create an object in BlueJ. Let me check the provided documents first. The documents are about content dispute resolution cases, specifically in the context of games and other web-related issues. There's a case study from the Korea Consumer Agency about an online game where a user's account was hacked, and another case about account suspension due to item trading. \n",
      "\n",
      "Wait, the user's question is about BlueJ programming environment. The documents provided don't mention anything related to BlueJ, object creation, or programming concepts. All the content is about legal cases and dispute resolutions in online games. \n",
      "\n",
      "Since the documents don't have information about BlueJ or object-oriented programming, I need to inform the user that the answer isn't available in the provided texts. The answer rules state that if the information isn't in the documents, I should say \"문서에서 찾을 수 없습니다\". \n",
      "\n",
      "So, the correct response here is to point out that the documents don't cover BlueJ's object creation methods. There's no mention of BlueJ's features, menus, buttons, or programming steps. Therefore, the answer must be that the information isn't present in the given materials.\n",
      "</think>\n",
      "\n",
      "문서에서 찾을 수 없습니다.  \n",
      "제공된 문서는 콘텐츠분쟁조정 사례 및 게임 관련 법리 분석에 관한 내용으로, BlueJ 프로그래밍 환경이나 객체 생성 방법에 대한 정보는 포함되어 있지 않습니다.\n",
      "\n",
      "\n",
      "질문  : 컴파일 오류가 발생했을 때 어떻게 확인할 수 있나요?\n",
      "<think>\n",
      "Okay, let's see. The user is asking how to check for compilation errors in BlueJ. But wait, the provided documents are about consumer dispute cases related to online games and service issues, not about programming errors or BlueJ specifically. The documents mention things like game item restoration, system errors, account bans, etc., but nothing about compilation errors or BlueJ's features.\n",
      "\n",
      "First, I need to confirm that the user's question is about BlueJ's compilation errors. The documents don't discuss BlueJ's error checking processes. The user might be confused or there's a misunderstanding. Since the documents are all about consumer disputes and not programming environments, I can't find any information related to compilation errors in BlueJ. \n",
      "\n",
      "So, according to the answer rules, I should state that the information isn't in the documents. The user might need to refer to BlueJ's documentation or other resources. But based strictly on the given text, there's no mention of compilation errors or how to check them. Therefore, the answer must indicate that the documents don't cover this topic.\n",
      "</think>\n",
      "\n",
      "문서에서 찾을 수 없습니다.  \n",
      "문서 내용은 게임 서비스 관련 소비자 분쟁 조정 사례 및 법리 분석에 초점이 맞춰져 있으며, **컴파일 오류 확인 방법**이나 **BlueJ 프로그래밍 환경의 오류 처리 절차**에 대한 정보는 포함되어 있지 않습니다.  \n",
      "따라서 제공된 문서를 기반으로 해당 질문에 대한 답변을 구성할 수 없습니다.\n"
     ]
    }
   ],
   "source": [
    "''' 변경된 부분'''\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "model = \"qwen3:8b\"\n",
    "url = \"http://localhost:11434\"\n",
    "embed_model= \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "# 임베딩 설정\n",
    "pythonembeddings = HuggingFaceEmbeddings(\n",
    "    model_name = embed_model,\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# llm 설정\n",
    "ollama_llm = ChatOllama(\n",
    "    model=model,\n",
    "    temperature=0.7,\n",
    "    num_predict=1000\n",
    ")\n",
    "\n",
    "# vector 설정\n",
    "vectorstore = FAISS.from_documents(chunks, pythonembeddings)\n",
    "python_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",        \n",
    "    search_kwargs={\"k\": 5}         \n",
    ")\n",
    "\n",
    "# Template 설정\n",
    "bluej_prompt_template = \"\"\"당신은 BlueJ 프로그래밍 환경 전문가입니다.\n",
    "아래 문서 내용을 바탕으로 정확하고 친절한 답변을 제공해주세요.\n",
    "\n",
    "문서 내용:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "답변 규칙:\n",
    "1. 문서 내용만을 근거로 답변하세요\n",
    "2. 단계별로 설명하세요  \n",
    "3. 구체적인 메뉴명, 버튼명을 포함하세요\n",
    "4. 문서에 없는 정보는 \"문서에서 찾을 수 없습니다\"라고 하세요\n",
    "\n",
    "답변:\"\"\"\n",
    "\n",
    "bluej_prompt = PromptTemplate(\n",
    "    template=bluej_prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "\n",
    "# 체인 설정\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ollama_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=python_retriever,\n",
    "    chain_type_kwargs={\"prompt\": bluej_prompt},\n",
    "    return_source_documents=True\n",
    ")\n",
    "test_questions = [\n",
    "    \"BlueJ에서 객체를 생성하는 방법은 무엇인가요?\",\n",
    "    \"컴파일 오류가 발생했을 때 어떻게 확인할 수 있나요?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\n\\n질문  : {question}\")\n",
    "    response = qa_chain.invoke({\"query\":question})\n",
    "    print(response[\"result\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-iuDoj6Sc-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
